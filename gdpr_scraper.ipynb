{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade die Webseite...\n",
      "ðŸ”„ Starte Scraping Ã¼ber alle Seiten...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Fortschritt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 254/254 [14:35<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Scraping abgeschlossen! 2534 EintrÃ¤ge gespeichert in 'enforcement_tracker_full_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from tqdm import tqdm  # Fortschrittsanzeige\n",
    "\n",
    "# WebDriver mit WebDriver-Manager automatisch verwalten\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Chrome-Optionen fÃ¼r stabilen Betrieb\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--no-sandbox\")  \n",
    "options.add_argument(\"--disable-dev-shm-usage\")  \n",
    "options.add_argument(\"--window-size=1920,1080\")  \n",
    "\n",
    "# WebDriver starten\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Ziel-URL\n",
    "URL = \"https://www.enforcementtracker.com/\"\n",
    "print(\"Lade die Webseite...\")\n",
    "driver.get(URL)\n",
    "time.sleep(5)\n",
    "\n",
    "# Warten, bis die Tabelle geladen ist\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "\n",
    "# Fortschrittsanzeige einrichten\n",
    "total_pages = 254  # Anzahl der Seiten\n",
    "data = []\n",
    "\n",
    "print(\"ðŸ”„ Starte Scraping Ã¼ber alle Seiten...\\n\")\n",
    "\n",
    "for page in tqdm(range(1, total_pages + 1), desc=\"Scraping Fortschritt\"):\n",
    "    # HTML holen und mit BeautifulSoup verarbeiten\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    table = soup.find(\"table\", id=\"penalties\")  # Zielt auf die richtige Tabelle\n",
    "\n",
    "    # Daten extrahieren\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\", class_=[\"odd\", \"even\"])  # Alle relevanten Zeilen holen\n",
    "\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")[1:]  # Ignoriere das erste leere `td`\n",
    "            if len(cols) >= 12:  # Mindestens 12 Spalten mÃ¼ssen vorhanden sein\n",
    "                data.append([\n",
    "                    cols[0].text.strip(),  # ETid\n",
    "                    cols[1].text.strip(),  # Country\n",
    "                    cols[2].text.strip(),  # Authority\n",
    "                    cols[3].text.strip(),  # Date of Decision\n",
    "                    cols[4].text.strip(),  # Fine [â‚¬]\n",
    "                    cols[5].text.strip(),  # Controller/Processor\n",
    "                    cols[6].text.strip(),  # Sector\n",
    "                    cols[7].text.strip(),  # Quoted Art.\n",
    "                    cols[8].text.strip(),  # Type\n",
    "                    cols[9].text.strip(),  # Summary\n",
    "                    cols[10].text.strip(), # Source\n",
    "                    cols[11].text.strip()  # Direct URL\n",
    "                ])\n",
    "\n",
    "    # Versuchen, zur nÃ¤chsten Seite zu navigieren\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"penalties_next\"))\n",
    "        )\n",
    "\n",
    "        # Klick erzwingen via JavaScript (um Blockierungen zu umgehen)\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(3)  # Warten, damit die neue Seite geladen wird\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nðŸš€ Letzte Seite erreicht oder Fehler: {e}\")\n",
    "        break\n",
    "\n",
    "# WebDriver schlieÃŸen\n",
    "driver.quit()\n",
    "\n",
    "# âœ… Umwandlung in DataFrame\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    \"ETid\", \"Country\", \"Authority\", \"Date of Decision\", \"Fine [â‚¬]\", \n",
    "    \"Controller/Processor\", \"Sector\", \"Quoted Art.\", \"Type\", \n",
    "    \"Summary\", \"Source\", \"Direct URL\"\n",
    "])\n",
    "\n",
    "# âœ… Speichern als CSV (ohne fehlerhaften Index)\n",
    "csv_path = \"enforcement_tracker_full_data.csv\"\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\nâœ… Scraping abgeschlossen! {len(df)} EintrÃ¤ge gespeichert in '{csv_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
